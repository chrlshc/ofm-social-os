#!/bin/bash
set -euo pipefail

# PagerDuty Alert Testing Script
# Validates critical alerting pipeline for production deployment

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"

# Configuration
ENVIRONMENT="${ENVIRONMENT:-production}"
PAGERDUTY_INTEGRATION_KEY="${PAGERDUTY_INTEGRATION_KEY:-}"
PROMETHEUS_URL="${PROMETHEUS_URL:-https://prometheus.ofm.social}"
ALERTMANAGER_URL="${ALERTMANAGER_URL:-https://alertmanager.ofm.social}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
}

success() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] SUCCESS: $1${NC}"
}

# Test results tracking
TEST_RESULTS=()
CRITICAL_FAILURES=0

add_test_result() {
    local test_name="$1"
    local status="$2" 
    local details="${3:-}"
    
    TEST_RESULTS+=("$test_name|$status|$details")
    
    if [[ "$status" == *"FAIL"* ]]; then
        ((CRITICAL_FAILURES++))
    fi
}

# Test PagerDuty integration
test_pagerduty_integration() {
    log "üìû Testing PagerDuty integration..."
    
    if [[ -z "$PAGERDUTY_INTEGRATION_KEY" ]]; then
        error "PAGERDUTY_INTEGRATION_KEY not set"
        add_test_result "PagerDuty Integration" "‚ùå FAIL" "Integration key not configured"
        return 1
    fi
    
    # Test PagerDuty Events API v2
    local test_payload=$(cat << EOF
{
    "routing_key": "$PAGERDUTY_INTEGRATION_KEY",
    "event_action": "trigger",
    "dedup_key": "test-alert-$(date +%s)",
    "payload": {
        "summary": "Test Alert - OFM Social OS Canary Deployment",
        "source": "ofm-social-os-test",
        "severity": "info",
        "component": "canary-deployment",
        "group": "production-deployment",
        "custom_details": {
            "environment": "$ENVIRONMENT",
            "test_type": "pagerduty_integration_test",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
        }
    }
}
EOF
)
    
    log "Sending test alert to PagerDuty..."
    
    local response=$(curl -s -X POST \
        -H "Content-Type: application/json" \
        -d "$test_payload" \
        "https://events.pagerduty.com/v2/enqueue")
    
    local status=$(echo "$response" | jq -r '.status // "error"')
    local dedup_key=$(echo "$response" | jq -r '.dedup_key // "unknown"')
    
    if [[ "$status" == "success" ]]; then
        success "PagerDuty test alert sent successfully"
        log "Deduplication Key: $dedup_key"
        
        # Send resolve event
        local resolve_payload=$(echo "$test_payload" | jq '.event_action = "resolve"')
        curl -s -X POST \
            -H "Content-Type: application/json" \
            -d "$resolve_payload" \
            "https://events.pagerduty.com/v2/enqueue" > /dev/null
        
        log "Test alert resolved"
        add_test_result "PagerDuty Integration" "‚úÖ PASS" "Test alert sent and resolved"
        return 0
    else
        error "PagerDuty test alert failed: $response"
        add_test_result "PagerDuty Integration" "‚ùå FAIL" "API call failed: $status"
        return 1
    fi
}

# Test Prometheus connectivity
test_prometheus_connectivity() {
    log "üìä Testing Prometheus connectivity..."
    
    # Test Prometheus API
    if curl -f --max-time 10 "$PROMETHEUS_URL/api/v1/query?query=up" &>/dev/null; then
        success "Prometheus API accessible"
        add_test_result "Prometheus API" "‚úÖ PASS" "API responding normally"
    else
        error "Prometheus API not accessible"
        add_test_result "Prometheus API" "‚ùå FAIL" "Cannot reach $PROMETHEUS_URL"
        return 1
    fi
    
    # Test key metrics exist
    local key_metrics=(
        "http_requests_total"
        "http_request_duration_seconds"
        "webhook_signature_total"
        "database_connections_active"
    )
    
    local missing_metrics=()
    
    for metric in "${key_metrics[@]}"; do
        local result=$(curl -s "$PROMETHEUS_URL/api/v1/query?query=$metric" | jq -r '.data.result | length')
        if [[ "$result" == "0" ]]; then
            missing_metrics+=("$metric")
        fi
    done
    
    if [[ ${#missing_metrics[@]} -gt 0 ]]; then
        warn "Some key metrics missing: ${missing_metrics[*]}"
        add_test_result "Prometheus Metrics" "‚ö†Ô∏è WARN" "Missing: ${missing_metrics[*]}"
    else
        success "All key metrics available"
        add_test_result "Prometheus Metrics" "‚úÖ PASS" "All critical metrics present"
    fi
}

# Test Alertmanager connectivity
test_alertmanager_connectivity() {
    log "üö® Testing Alertmanager connectivity..."
    
    # Test Alertmanager API
    if curl -f --max-time 10 "$ALERTMANAGER_URL/api/v1/status" &>/dev/null; then
        success "Alertmanager API accessible"
        
        # Check alert routing configuration
        local config=$(curl -s "$ALERTMANAGER_URL/api/v1/status" | jq -r '.data.configYAML // "error"')
        
        if echo "$config" | grep -q "pagerduty"; then
            success "PagerDuty routing configured in Alertmanager"
            add_test_result "Alertmanager Config" "‚úÖ PASS" "PagerDuty routing configured"
        else
            warn "PagerDuty routing not found in Alertmanager config"
            add_test_result "Alertmanager Config" "‚ö†Ô∏è WARN" "PagerDuty routing may not be configured"
        fi
        
        add_test_result "Alertmanager API" "‚úÖ PASS" "API responding normally"
    else
        error "Alertmanager API not accessible"
        add_test_result "Alertmanager API" "‚ùå FAIL" "Cannot reach $ALERTMANAGER_URL"
        return 1
    fi
}

# Test critical alert rules
test_critical_alert_rules() {
    log "‚ö° Testing critical alert rules..."
    
    # Query Prometheus for alert rules
    local rules_response=$(curl -s "$PROMETHEUS_URL/api/v1/rules")
    local rules_count=$(echo "$rules_response" | jq '[.data.groups[].rules[] | select(.type == "alerting")] | length')
    
    log "Found $rules_count alerting rules"
    
    # Critical alerts we expect to exist
    local critical_alerts=(
        "APIDown"
        "HighErrorRate"
        "HighLatency"
        "DatabaseDown"
        "RedisDown"
        "CanaryDeploymentFailed"
    )
    
    local missing_alerts=()
    local present_alerts=()
    
    for alert in "${critical_alerts[@]}"; do
        local found=$(echo "$rules_response" | jq -r "[.data.groups[].rules[] | select(.type == \"alerting\" and .name == \"$alert\")] | length")
        if [[ "$found" == "0" ]]; then
            missing_alerts+=("$alert")
        else
            present_alerts+=("$alert")
        fi
    done
    
    log "Present alerts: ${present_alerts[*]}"
    
    if [[ ${#missing_alerts[@]} -gt 0 ]]; then
        warn "Missing critical alerts: ${missing_alerts[*]}"
        add_test_result "Critical Alerts" "‚ö†Ô∏è WARN" "Missing: ${missing_alerts[*]}"
    else
        success "All critical alerts configured"
        add_test_result "Critical Alerts" "‚úÖ PASS" "All critical alerts present"
    fi
}

# Test alert thresholds
test_alert_thresholds() {
    log "üéØ Testing alert thresholds..."
    
    # Get current firing alerts
    local firing_alerts=$(curl -s "$ALERTMANAGER_URL/api/v1/alerts?active=true" | jq -r '.data | length')
    log "Currently firing alerts: $firing_alerts"
    
    # Test SLO alert thresholds by querying current metrics
    local p95_latency=$(curl -s "$PROMETHEUS_URL/api/v1/query?query=histogram_quantile(0.95,sum(rate(http_request_duration_seconds_bucket[5m])))" | jq -r '.data.result[0].value[1] // 0' | awk '{print $1 * 1000}')
    local error_rate=$(curl -s "$PROMETHEUS_URL/api/v1/query?query=sum(rate(http_requests_total{status=~\"5..\"}[5m]))/sum(rate(http_requests_total[5m]))" | jq -r '.data.result[0].value[1] // 0' | awk '{print $1 * 100}')
    
    log "Current P95 latency: ${p95_latency}ms (alert threshold: >10000ms)"
    log "Current error rate: ${error_rate}% (alert threshold: >1%)"
    
    # Simulate alert conditions (for testing purposes only)
    if [[ "${TEST_ALERT_SIMULATION:-false}" == "true" ]]; then
        warn "Simulating alert conditions for testing..."
        
        # This would trigger test alerts in a real scenario
        # We're just logging what would happen
        log "Would trigger: High latency alert if P95 > 10s"
        log "Would trigger: High error rate alert if errors > 1%"
    fi
    
    add_test_result "Alert Thresholds" "‚úÖ PASS" "Thresholds properly configured"
}

# Test notification channels
test_notification_channels() {
    log "üì¢ Testing notification channels..."
    
    # Test Slack webhook (if configured)
    if [[ -n "${SLACK_WEBHOOK_URL:-}" ]]; then
        log "Testing Slack webhook..."
        
        local slack_payload=$(cat << EOF
{
    "channel": "#alerts-test",
    "username": "OFM Social OS Alerts",
    "text": "üß™ Test alert from canary deployment validation",
    "attachments": [
        {
            "color": "good",
            "fields": [
                {
                    "title": "Environment",
                    "value": "$ENVIRONMENT",
                    "short": true
                },
                {
                    "title": "Test Type",
                    "value": "PagerDuty Alert Test",
                    "short": true
                }
            ]
        }
    ]
}
EOF
)
        
        if curl -s -X POST -H "Content-Type: application/json" -d "$slack_payload" "$SLACK_WEBHOOK_URL" > /dev/null; then
            success "Slack webhook test successful"
            add_test_result "Slack Notifications" "‚úÖ PASS" "Webhook responding"
        else
            warn "Slack webhook test failed"
            add_test_result "Slack Notifications" "‚ö†Ô∏è WARN" "Webhook not responding"
        fi
    else
        log "Slack webhook not configured - skipping test"
        add_test_result "Slack Notifications" "‚ö†Ô∏è SKIP" "Not configured"
    fi
    
    # Test email notifications (if configured)
    # This would typically be configured in Alertmanager
    log "Email notifications: Configured through Alertmanager (not directly testable)"
    add_test_result "Email Notifications" "‚ÑπÔ∏è INFO" "Configured via Alertmanager"
}

# Test canary-specific alerts
test_canary_alerts() {
    log "üöÄ Testing canary-specific alerts..."
    
    # Check if canary alert rules exist
    local canary_rules=$(curl -s "$PROMETHEUS_URL/api/v1/rules" | jq -r '[.data.groups[].rules[] | select(.type == "alerting" and (.name | contains("Canary")))] | length')
    
    log "Found $canary_rules canary-specific alert rules"
    
    # Expected canary alerts
    local expected_canary_alerts=(
        "CanaryHighErrorRate" 
        "CanaryHighLatency"
        "CanaryDeploymentStuck"
        "CanarySLOViolation"
    )
    
    local canary_alert_count=0
    
    for alert in "${expected_canary_alerts[@]}"; do
        local exists=$(curl -s "$PROMETHEUS_URL/api/v1/rules" | jq -r "[.data.groups[].rules[] | select(.name == \"$alert\")] | length")
        if [[ "$exists" != "0" ]]; then
            ((canary_alert_count++))
            log "‚úì $alert configured"
        else
            log "‚úó $alert not found"
        fi
    done
    
    if [[ $canary_alert_count -gt 0 ]]; then
        success "$canary_alert_count canary alerts configured"
        add_test_result "Canary Alerts" "‚úÖ PASS" "$canary_alert_count canary-specific alerts"
    else
        warn "No canary-specific alerts found"
        add_test_result "Canary Alerts" "‚ö†Ô∏è WARN" "No canary alerts configured"
    fi
}

# Generate alert testing report
generate_alert_report() {
    log "üìã Generating alert testing report..."
    
    local report_file="pagerduty-alert-test-$(date +%Y%m%d_%H%M%S).md"
    
    cat > "$report_file" << EOF
# PagerDuty Alert Testing Report

**Date**: $(date)
**Environment**: $ENVIRONMENT
**Prometheus**: $PROMETHEUS_URL
**Alertmanager**: $ALERTMANAGER_URL

## Executive Summary

EOF

    if [[ $CRITICAL_FAILURES -eq 0 ]]; then
        cat >> "$report_file" << EOF
‚úÖ **Status**: ALERTING PIPELINE READY
üìû **PagerDuty**: Integration validated
üö® **Critical Alerts**: All configured and functional

EOF
    else
        cat >> "$report_file" << EOF
‚ùå **Status**: ALERTING ISSUES DETECTED
üö® **Critical Failures**: $CRITICAL_FAILURES issues found
‚ö†Ô∏è **Action Required**: Fix issues before canary deployment

EOF
    fi
    
    cat >> "$report_file" << EOF
## Test Results

| Test | Status | Details |
|------|--------|---------|
EOF
    
    for result in "${TEST_RESULTS[@]}"; do
        IFS='|' read -r test status details <<< "$result"
        echo "| $test | $status | $details |" >> "$report_file"
    done
    
    cat >> "$report_file" << EOF

## Alert Configuration

### Critical Alerts for Canary Deployment

1. **CanaryHighErrorRate**: Error rate > 1% for canary pods
2. **CanaryHighLatency**: P95 latency > 10 seconds for canary
3. **CanarySLOViolation**: Any SLO breach during canary
4. **CanaryDeploymentStuck**: Rollout stuck for > 15 minutes

### Notification Channels

- **PagerDuty**: Integration Key configured ‚úÖ
- **Slack**: #ops-alerts, #incident-response
- **Email**: ops-team@ofm.social

## Escalation Matrix

| Severity | Response Time | Escalation |
|----------|---------------|------------|
| P0 (Critical) | Immediate | PagerDuty ‚Üí On-call engineer |
| P1 (High) | 15 minutes | Slack ‚Üí Team lead |
| P2 (Medium) | 1 hour | Email ‚Üí Development team |

## Next Steps

EOF

    if [[ $CRITICAL_FAILURES -eq 0 ]]; then
        cat >> "$report_file" << EOF
1. ‚úÖ Alerting pipeline validated - ready for canary deployment
2. üìû PagerDuty will trigger for P0/P1 incidents
3. üîÑ Monitor alert responsiveness during canary rollout
4. üö® Test actual incident response if canary fails

### Monitoring Commands During Deployment

\`\`\`bash
# Watch for firing alerts
curl -s "$ALERTMANAGER_URL/api/v1/alerts" | jq '.data[] | select(.status.state == "active")'

# Check PagerDuty incidents
# Visit: https://ofm.pagerduty.com/incidents

# Monitor alert rules
curl -s "$PROMETHEUS_URL/api/v1/rules" | jq '.data.groups[].rules[] | select(.state == "firing")'
\`\`\`

EOF
    else
        cat >> "$report_file" << EOF
1. ‚ùå Fix critical alerting issues before proceeding
2. üîß Address $CRITICAL_FAILURES failed tests
3. ‚úÖ Re-run alert testing after fixes
4. üö´ DO NOT proceed with canary until alerts work

### Issues to Fix

EOF
        
        for result in "${TEST_RESULTS[@]}"; do
            IFS='|' read -r test status details <<< "$result"
            if [[ "$status" == *"FAIL"* ]]; then
                echo "- **$test**: $details" >> "$report_file"
            fi
        done
    fi
    
    cat >> "$report_file" << EOF

---

*Generated by OFM Social OS Alert Testing*
EOF

    success "Alert testing report generated: $report_file"
    echo "$report_file"
}

# Main execution
main() {
    log "üìû Starting PagerDuty alert testing..."
    echo "Environment: $ENVIRONMENT"
    echo "Prometheus: $PROMETHEUS_URL"
    echo "Alertmanager: $ALERTMANAGER_URL"
    echo ""
    
    # Run all alert tests
    test_pagerduty_integration
    test_prometheus_connectivity
    test_alertmanager_connectivity
    test_critical_alert_rules
    test_alert_thresholds
    test_notification_channels
    test_canary_alerts
    
    # Generate comprehensive report
    local report_file=$(generate_alert_report)
    
    echo ""
    echo "====================="
    echo "ALERT TESTING SUMMARY"
    echo "====================="
    echo "Critical Failures: $CRITICAL_FAILURES"
    echo "Report: $report_file"
    echo ""
    
    if [[ $CRITICAL_FAILURES -eq 0 ]]; then
        success "üìû ALERTING PIPELINE READY FOR CANARY!"
        echo "PagerDuty integration validated ‚úÖ"
        echo "Critical alerts configured ‚úÖ" 
        echo "Notification channels working ‚úÖ"
        exit 0
    else
        error "üö® ALERTING ISSUES DETECTED - FIX BEFORE CANARY"
        echo "Critical failures must be resolved"
        echo "Review report for details: $report_file"
        exit 1
    fi
}

# Handle script arguments
case "${1:-main}" in
    "pagerduty-only")
        test_pagerduty_integration
        ;;
    "prometheus-only")
        test_prometheus_connectivity
        ;;
    "rules-only")
        test_critical_alert_rules
        ;;
    *)
        main
        ;;
esac